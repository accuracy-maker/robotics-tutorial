{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tutorial from Youtube:\n",
    "\n",
    "https://www.youtube.com/@MachineLearningwithPhil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions,\n",
    "                                   input_dims=input_dims,\n",
    "                                   fc1_dims=256, fc2_dims=256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(\n",
    "                self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(\n",
    "                self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(\n",
    "                self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 14.00 average score 14.00 epsilon 1.00\n",
      "episode  1 score 15.00 average score 14.50 epsilon 1.00\n",
      "episode  2 score 14.00 average score 14.33 epsilon 1.00\n",
      "episode  3 score 14.00 average score 14.25 epsilon 1.00\n",
      "episode  4 score 18.00 average score 15.00 epsilon 0.99\n",
      "episode  5 score 15.00 average score 15.00 epsilon 0.99\n",
      "episode  6 score 20.00 average score 15.71 epsilon 0.98\n",
      "episode  7 score 17.00 average score 15.88 epsilon 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/fs8f0mrx5c7d2_ny5vdyy0ym0000gp/T/ipykernel_43689/956477620.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  state = T.tensor([observation]).to(self.Q_eval.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  8 score 12.00 average score 15.44 epsilon 0.96\n",
      "episode  9 score 15.00 average score 15.40 epsilon 0.95\n",
      "episode  10 score 23.00 average score 16.09 epsilon 0.94\n",
      "episode  11 score 36.00 average score 17.75 epsilon 0.93\n",
      "episode  12 score 73.00 average score 22.00 epsilon 0.89\n",
      "episode  13 score 34.00 average score 22.86 epsilon 0.87\n",
      "episode  14 score 24.00 average score 22.93 epsilon 0.86\n",
      "episode  15 score 19.00 average score 22.69 epsilon 0.85\n",
      "episode  16 score 26.00 average score 22.88 epsilon 0.84\n",
      "episode  17 score 15.00 average score 22.44 epsilon 0.83\n",
      "episode  18 score 17.00 average score 22.16 epsilon 0.82\n",
      "episode  19 score 14.00 average score 21.75 epsilon 0.81\n",
      "episode  20 score 43.00 average score 22.76 epsilon 0.79\n",
      "episode  21 score 24.00 average score 22.82 epsilon 0.78\n",
      "episode  22 score 23.00 average score 22.83 epsilon 0.77\n",
      "episode  23 score 25.00 average score 22.92 epsilon 0.76\n",
      "episode  24 score 49.00 average score 23.96 epsilon 0.73\n",
      "episode  25 score 25.00 average score 24.00 epsilon 0.72\n",
      "episode  26 score 45.00 average score 24.78 epsilon 0.70\n",
      "episode  27 score 21.00 average score 24.64 epsilon 0.69\n",
      "episode  28 score 31.00 average score 24.86 epsilon 0.67\n",
      "episode  29 score 29.00 average score 25.00 epsilon 0.66\n",
      "episode  30 score 33.00 average score 25.26 epsilon 0.64\n",
      "episode  31 score 11.00 average score 24.81 epsilon 0.63\n",
      "episode  32 score 33.00 average score 25.06 epsilon 0.62\n",
      "episode  33 score 53.00 average score 25.88 epsilon 0.59\n",
      "episode  34 score 52.00 average score 26.63 epsilon 0.57\n",
      "episode  35 score 21.00 average score 26.47 epsilon 0.56\n",
      "episode  36 score 57.00 average score 27.30 epsilon 0.53\n",
      "episode  37 score 53.00 average score 27.97 epsilon 0.50\n",
      "episode  38 score 28.00 average score 27.97 epsilon 0.49\n",
      "episode  39 score 157.00 average score 31.20 epsilon 0.41\n",
      "episode  40 score 125.00 average score 33.49 epsilon 0.35\n",
      "episode  41 score 154.00 average score 36.36 epsilon 0.27\n",
      "episode  42 score 155.00 average score 39.12 epsilon 0.19\n",
      "episode  43 score 107.00 average score 40.66 epsilon 0.14\n",
      "episode  44 score 129.00 average score 42.62 epsilon 0.07\n",
      "episode  45 score 213.00 average score 46.33 epsilon 0.01\n",
      "episode  46 score 111.00 average score 47.70 epsilon 0.01\n",
      "episode  47 score 163.00 average score 50.10 epsilon 0.01\n",
      "episode  48 score 85.00 average score 50.82 epsilon 0.01\n",
      "episode  49 score 89.00 average score 51.58 epsilon 0.01\n",
      "episode  50 score 87.00 average score 52.27 epsilon 0.01\n",
      "episode  51 score 179.00 average score 54.71 epsilon 0.01\n",
      "episode  52 score 188.00 average score 57.23 epsilon 0.01\n",
      "episode  53 score 129.00 average score 58.56 epsilon 0.01\n",
      "episode  54 score 280.00 average score 62.58 epsilon 0.01\n",
      "episode  55 score 172.00 average score 64.54 epsilon 0.01\n",
      "episode  56 score 117.00 average score 65.46 epsilon 0.01\n",
      "episode  57 score 211.00 average score 67.97 epsilon 0.01\n",
      "episode  58 score 179.00 average score 69.85 epsilon 0.01\n",
      "episode  59 score 132.00 average score 70.88 epsilon 0.01\n",
      "episode  60 score 198.00 average score 72.97 epsilon 0.01\n",
      "episode  61 score 174.00 average score 74.60 epsilon 0.01\n",
      "episode  62 score 138.00 average score 75.60 epsilon 0.01\n",
      "episode  63 score 121.00 average score 76.31 epsilon 0.01\n",
      "episode  64 score 120.00 average score 76.98 epsilon 0.01\n",
      "episode  65 score 216.00 average score 79.09 epsilon 0.01\n",
      "episode  66 score 128.00 average score 79.82 epsilon 0.01\n",
      "episode  67 score 138.00 average score 80.68 epsilon 0.01\n",
      "episode  68 score 174.00 average score 82.03 epsilon 0.01\n",
      "episode  69 score 188.00 average score 83.54 epsilon 0.01\n",
      "episode  70 score 223.00 average score 85.51 epsilon 0.01\n",
      "episode  71 score 174.00 average score 86.74 epsilon 0.01\n",
      "episode  72 score 171.00 average score 87.89 epsilon 0.01\n",
      "episode  73 score 175.00 average score 89.07 epsilon 0.01\n",
      "episode  74 score 268.00 average score 91.45 epsilon 0.01\n",
      "episode  75 score 145.00 average score 92.16 epsilon 0.01\n",
      "episode  76 score 159.00 average score 93.03 epsilon 0.01\n",
      "episode  77 score 139.00 average score 93.62 epsilon 0.01\n",
      "episode  78 score 141.00 average score 94.22 epsilon 0.01\n",
      "episode  79 score 188.00 average score 95.39 epsilon 0.01\n",
      "episode  80 score 288.00 average score 97.77 epsilon 0.01\n",
      "episode  81 score 205.00 average score 99.07 epsilon 0.01\n",
      "episode  82 score 134.00 average score 99.49 epsilon 0.01\n",
      "episode  83 score 203.00 average score 100.73 epsilon 0.01\n",
      "episode  84 score 362.00 average score 103.80 epsilon 0.01\n",
      "episode  85 score 179.00 average score 104.67 epsilon 0.01\n",
      "episode  86 score 268.00 average score 106.55 epsilon 0.01\n",
      "episode  87 score 200.00 average score 107.61 epsilon 0.01\n",
      "episode  88 score 181.00 average score 108.44 epsilon 0.01\n",
      "episode  89 score 157.00 average score 108.98 epsilon 0.01\n",
      "episode  90 score 178.00 average score 109.74 epsilon 0.01\n",
      "episode  91 score 169.00 average score 110.38 epsilon 0.01\n",
      "episode  92 score 214.00 average score 111.49 epsilon 0.01\n",
      "episode  93 score 188.00 average score 112.31 epsilon 0.01\n",
      "episode  94 score 226.00 average score 113.51 epsilon 0.01\n",
      "episode  95 score 235.00 average score 114.77 epsilon 0.01\n",
      "episode  96 score 448.00 average score 118.21 epsilon 0.01\n",
      "episode  97 score 167.00 average score 118.70 epsilon 0.01\n",
      "episode  98 score 255.00 average score 120.08 epsilon 0.01\n",
      "episode  99 score 190.00 average score 120.78 epsilon 0.01\n",
      "episode  100 score 199.00 average score 122.63 epsilon 0.01\n",
      "episode  101 score 273.00 average score 125.21 epsilon 0.01\n",
      "episode  102 score 279.00 average score 127.86 epsilon 0.01\n",
      "episode  103 score 287.00 average score 130.59 epsilon 0.01\n",
      "episode  104 score 183.00 average score 132.24 epsilon 0.01\n",
      "episode  105 score 250.00 average score 134.59 epsilon 0.01\n",
      "episode  106 score 199.00 average score 136.38 epsilon 0.01\n",
      "episode  107 score 251.00 average score 138.72 epsilon 0.01\n",
      "episode  108 score 268.00 average score 141.28 epsilon 0.01\n",
      "episode  109 score 277.00 average score 143.90 epsilon 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gaohaitao/robotics-tutorial/DQN.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     agent\u001b[39m.\u001b[39mstore_transition(observation, action, reward, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                             observation_, done)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     observation \u001b[39m=\u001b[39m observation_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(score)\n",
      "\u001b[1;32m/Users/gaohaitao/robotics-tutorial/DQN.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m q_next[terminal_batch] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m q_target \u001b[39m=\u001b[39m reward_batch \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma\u001b[39m*\u001b[39mT\u001b[39m.\u001b[39mmax(q_next, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQ_eval\u001b[39m.\u001b[39;49mloss(q_target, q_eval)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_eval\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gaohaitao/robotics-tutorial/DQN.ipynb#W1sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_eval\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/robotics/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/robotics/lib/python3.10/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/robotics/lib/python3.10/site-packages/torch/nn/functional.py:3295\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[0;32m-> 3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=2, eps_end=0.01,\n",
    "                input_dims=[4], lr=0.001)\n",
    "scores, eps_history = [], []\n",
    "n_games = 500\n",
    "\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation,info = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        done = terminated or truncated\n",
    "        agent.store_transition(observation, action, reward, \n",
    "                                observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
